{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rI47UwdgTWgk",
        "outputId": "2acca93d-1c05-4e93-ea07-5d69d48bbf5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nibabel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CRG31hT1anmT",
        "outputId": "c3bfdf53-25b7-4f06-efcc-045cacca9cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Collecting nibabel\n",
            "  Downloading nibabel-5.2.1-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from nibabel) (1.25.2)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.10/dist-packages (from nibabel) (24.0)\n",
            "Installing collected packages: nibabel\n",
            "  Attempting uninstall: nibabel\n",
            "    Found existing installation: nibabel 4.0.2\n",
            "    Uninstalling nibabel-4.0.2:\n",
            "      Successfully uninstalled nibabel-4.0.2\n",
            "Successfully installed nibabel-5.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkQWea12Eb3i"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import SimpleITK as sitk\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torchvision.transforms import Resize\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Lambda, Compose\n",
        "from scipy.ndimage import zoom\n",
        "import psutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n"
      ],
      "metadata": {
        "id": "k9lRlbcng0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing and saving the original files as nii.gz"
      ],
      "metadata": {
        "id": "iLIMYHB1TdM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Directory containing the .mhd files\n",
        "# input_dir = '/content/drive/MyDrive/seg_lung_luna'\n",
        "# output_dir = '/content/drive/MyDrive/Dataset'\n",
        "\n",
        "# # Reference size and spacing for all images\n",
        "# reference_size = [256, 256, 256]  # Example reference size\n",
        "# reference_spacing = [1.0, 1.0, 1.0]  # Example reference spacing\n",
        "\n",
        "# # List all .mhd files in the directory\n",
        "# mhd_files = glob.glob(os.path.join(input_dir, '*.mhd'))\n",
        "\n",
        "# def process_mhd_file(mhd_file):\n",
        "#     try:\n",
        "#         # Read the .mhd file\n",
        "#         img = sitk.ReadImage(mhd_file)\n",
        "#         # print(f\"Processing {mhd_file}\")\n",
        "\n",
        "#         # Configure the resampler to use the new transformation\n",
        "#         resampler = sitk.ResampleImageFilter()\n",
        "#         resampler.SetSize(reference_size)\n",
        "#         resampler.SetOutputSpacing(reference_spacing)\n",
        "#         resampler.SetOutputOrigin(img.GetOrigin())\n",
        "#         resampler.SetOutputDirection(img.GetDirection())\n",
        "#         resampler.SetInterpolator(sitk.sitkLinear)\n",
        "\n",
        "#         # Execute the resampling\n",
        "#         resampled_img = resampler.Execute(img)\n",
        "\n",
        "#         # Convert the resampled image to a NumPy array\n",
        "#         volume = sitk.GetArrayFromImage(resampled_img)\n",
        "\n",
        "#         # Normalize the volume based on the given HU values\n",
        "#         min_hu = -1000\n",
        "#         max_hu = 400\n",
        "#         volume_normalized = (volume - min_hu) / (max_hu - min_hu)\n",
        "\n",
        "#         # Prepare the affine matrix\n",
        "#         affine_matrix = np.eye(4)  # Identity matrix for 4x4\n",
        "\n",
        "#         # Save the preprocessed volume as a compressed .nii.gz file\n",
        "#         output_filename = os.path.splitext(os.path.basename(mhd_file))[0] + '.nii.gz'\n",
        "#         output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "#         nii_img = nib.Nifti1Image(volume_normalized, affine_matrix)\n",
        "#         nib.save(nii_img, output_path)  # Specify compress=True for gzip compression\n",
        "#         with open(output_path, 'rb') as f_in:\n",
        "#           with gzip.open(output_path + '.gz', 'wb') as f_out:\n",
        "#               f_out.writelines(f_in)\n",
        "\n",
        "#         print(f\"Processed {mhd_file} and saved as {output_path}\")\n",
        "\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing {mhd_file}: {e}\")\n",
        "\n",
        "# # Use ThreadPoolExecutor for parallel processing\n",
        "# with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "#     executor.map(process_mhd_file, mhd_files)\n"
      ],
      "metadata": {
        "id": "CTSsxuN4qBQS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the custom dataset\n"
      ],
      "metadata": {
        "id": "65NvZBH-gTqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CTMaskedDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, target_size=(128, 128)):\n",
        "        self.root_dir = root_dir\n",
        "        self.target_size = target_size\n",
        "        self.scan_folders = [f for f in os.listdir(self.root_dir) if f.endswith('_extracted.nii.gz') or f.endswith('_masked_extracted.nii.gz')]\n",
        "        print(f\"Found {len(self.scan_folders)} NIFTI files in {self.root_dir}\")\n",
        "        self.transform = transform\n",
        "        self.max_slices = self.calculate_max_slices()\n",
        "\n",
        "    def calculate_max_slices(self):\n",
        "        num_slices_per_scan = []\n",
        "        all_files = os.listdir(self.root_dir)\n",
        "        nii_files = [f for f in all_files if f.endswith('.nii.gz')]\n",
        "        for nii_file in nii_files:\n",
        "            num_slices = len(self.load_nifti(os.path.join(self.root_dir, nii_file)))\n",
        "            num_slices_per_scan.append(num_slices)\n",
        "        return max(num_slices_per_scan)\n",
        "\n",
        "    def __len__(self):\n",
        "        valid_count = 0\n",
        "        for nifti_file in self.scan_folders:\n",
        "            if '_masked_extracted.nii.gz' in nifti_file:\n",
        "                valid_count += 1\n",
        "        print(\"Valid count:\", valid_count)\n",
        "        return valid_count\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        scan_path = self.root_dir\n",
        "        all_files = os.listdir(scan_path)\n",
        "        nii_files = [f for f in all_files if f.endswith('_extracted.nii.gz')]\n",
        "        masked_files = [f for f in all_files if f.endswith('_masked_extracted.nii.gz')]\n",
        "        file_pairs = list(zip(nii_files, masked_files))\n",
        "        original_file, masked_file = file_pairs[idx]\n",
        "        original_file_path = os.path.join(scan_path, original_file)\n",
        "        masked_file_path = os.path.join(scan_path, masked_file)\n",
        "        if not os.path.isfile(original_file_path) or not os.path.isfile(masked_file_path):\n",
        "            print(f\"Missing file for {original_file}, returning zeros for missing data.\")\n",
        "            image = np.zeros_like(np.random.rand(1, 128, 128))\n",
        "            mask = np.zeros_like(np.random.rand(1, 128, 128))\n",
        "        else:\n",
        "            image = torch.from_numpy(self.load_nifti(original_file_path)).float()\n",
        "            mask = torch.from_numpy(self.load_nifti(masked_file_path)).float()\n",
        "\n",
        "        padding_needed = self.target_size[1] - mask.shape[1]\n",
        "        padding_needed = max(0, padding_needed)\n",
        "        padded_image = F.pad(image, (0, padding_needed, 0, 0), \"constant\", 0)\n",
        "        padded_mask = F.pad(mask, (0, padding_needed, 0, 0), \"constant\", 0)\n",
        "        padded_mask = self.normalize_mask(padded_mask)\n",
        "\n",
        "        if self.transform:\n",
        "            padded_image = self.transform(padded_image)\n",
        "            padded_mask = self.transform(padded_mask)\n",
        "        return image, mask\n",
        "\n",
        "    def load_nifti(self, nifti_file):\n",
        "        nifti_data = nib.load(nifti_file)\n",
        "        nifti_array = np.array(nifti_data.get_fdata())\n",
        "        return nifti_array\n",
        "\n",
        "    def normalize_mask(self, mask_array):\n",
        "        mask_min = mask_array.min()\n",
        "        mask_max = mask_array.max()\n",
        "        mask_array = (mask_array - mask_min) / (mask_max - mask_min)\n",
        "        mask_array = mask_array.type(torch.uint8)\n",
        "        return mask_array\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    images = []\n",
        "    masks = []\n",
        "    for item in batch:\n",
        "        image, mask = item\n",
        "        # Resize both image and mask to a fixed size\n",
        "        resize = Resize((128, 128))\n",
        "        image = resize(image.unsqueeze(0)).squeeze(0)\n",
        "        mask = resize(mask.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "        # Check if the mask needs padding to match the target size\n",
        "        padding_needed = 128 - mask.shape[1]\n",
        "        padding_needed = max(0, padding_needed)\n",
        "        mask_padded = F.pad(mask, (0, padding_needed, 0, 0), \"constant\", 0)\n",
        "\n",
        "        images.append(image)\n",
        "        masks.append(mask_padded)\n",
        "\n",
        "    # Stack images and masks\n",
        "    images = torch.stack(images, dim=0)\n",
        "    masks = torch.stack(masks, dim=0)\n",
        "\n",
        "    return images, masks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TbcH2ohytMTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2C_Rkh5tNxb",
        "outputId": "34db1721-a870-4524-b80a-27a7e35c1841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Res2Net definition"
      ],
      "metadata": {
        "id": "cLP8gIIbg2u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Res2NetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, scale=4):\n",
        "        super(Res2NetBlock, self).__init__()\n",
        "        self.scale = scale\n",
        "\n",
        "        # 1x1x1 conv to reduce input channels to scale * out_channels\n",
        "        reduced_channels = scale * out_channels\n",
        "        self.conv1x1x1 = nn.Conv3d(in_channels, reduced_channels, kernel_size=1)\n",
        "\n",
        "        # Grouped convolutions within Res2Net block\n",
        "        self.conv3x3x3_groups = nn.ModuleList([\n",
        "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, groups=scale)\n",
        "            for _ in range(scale)\n",
        "        ])\n",
        "\n",
        "        # 3D Squeeze-and-Excitation (SE) block\n",
        "        self.se_block = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool3d(1),\n",
        "            nn.Conv3d(reduced_channels, reduced_channels // 16, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(reduced_channels // 16, reduced_channels, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply 1x1x1 convolution\n",
        "        x = self.conv1x1x1(x)\n",
        "\n",
        "        # Split into groups and apply grouped convolutions\n",
        "        x_splits = torch.chunk(x, self.scale, dim=1)\n",
        "        out_splits = [conv(split) for conv, split in zip(self.conv3x3x3_groups, x_splits)]\n",
        "        out = torch.cat(out_splits, dim=1)\n",
        "\n",
        "        # Apply SE block for channel-wise attention\n",
        "        se_weights = self.se_block(out)\n",
        "        out = out * se_weights\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zzuv9SEAg0N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UNet definition"
      ],
      "metadata": {
        "id": "7zSQsm0Dg57m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import gc\n",
        "\n",
        "class ModifiedUNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, base_channels=256):\n",
        "        super(ModifiedUNet, self).__init__()\n",
        "        self.encoder1 = self.conv_block(in_channels, base_channels)\n",
        "        self.encoder2 = self.res2net_block(base_channels, base_channels * 2)\n",
        "        self.encoder3 = self.res2net_block(base_channels * 2, base_channels * 4)\n",
        "        self.encoder4 = self.conv_block(base_channels * 4, base_channels * 8)\n",
        "\n",
        "\n",
        "        self.decoder3 = self.conv_block(base_channels * 8, base_channels * 4)\n",
        "        self.decoder2 = self.res2net_block(base_channels * 4 + base_channels * 4, base_channels * 2)\n",
        "        self.decoder1 = self.conv_block(base_channels * 2 + base_channels * 2, base_channels)\n",
        "        self.final_conv = nn.Conv3d(base_channels, out_channels, kernel_size=1)\n",
        "        print(f\"GPU memory allocated after operations: {torch.cuda.memory_allocated()} bytes\")\n",
        "\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def res2net_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            Res2NetBlock(in_channels, out_channels),  # Assuming Res2NetBlock is defined elsewhere\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use checkpoint for the entire forward pass\n",
        "        x = checkpoint(self.encoder1, x)\n",
        "        x = checkpoint(self.encoder2, x)\n",
        "        x = checkpoint(self.encoder3, x)\n",
        "        x = checkpoint(self.encoder4, x)\n",
        "\n",
        "        x = checkpoint(self.decoder3, x)\n",
        "        x = torch.cat([x, checkpoint(self.encoder3, x)], dim=1)  # Concatenate with encoder3 output\n",
        "        x = checkpoint(self.decoder2, x)\n",
        "        x = torch.cat([x, checkpoint(self.encoder2, x)], dim=1)  # Concatenate with encoder2 output\n",
        "        x = checkpoint(self.decoder1, x)\n",
        "\n",
        "        out = self.final_conv(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "GlMLPamfxLDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "ppjRr8ACquOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import nibabel as nib\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import Lambda, Compose, ToTensor\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from torch.optim import Adam\n",
        "import psutil\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import gc\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def resize_np(arr, size):\n",
        "    scale_factor = min(size[0] / arr.shape[0], size[1] / arr.shape[1])\n",
        "\n",
        "    # Initialize an empty array to hold the resized image\n",
        "    resized_arr = np.empty((size[0], size[1], arr.shape[-1]))\n",
        "\n",
        "    # Resize each channel individually\n",
        "    for ch in range(arr.shape[-1]):\n",
        "        resized_arr[:, :, ch] = zoom(arr[:, :, ch], (scale_factor, scale_factor), order=1)\n",
        "\n",
        "    return resized_arr\n",
        "\n",
        "# Define your dataset path\n",
        "root_dir = '/content/drive/MyDrive/ct_scan'\n",
        "\n",
        "# Initialize your dataset\n",
        "dataset = CTMaskedDataset(root_dir=root_dir, transform=Compose([Lambda(lambda x: resize_np(x, (128, 128))), ToTensor()]))\n",
        "\n",
        "# Split the dataset into training, testing, and validation sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = int(0.15 * len(dataset))\n",
        "val_size = len(dataset) - train_size - test_size\n",
        "train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])\n",
        "\n",
        "# Create separate DataLoaders for each phase\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn, pin_memory=True)\n",
        "print(f\"Number of workers: {train_loader.num_workers}\")\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn, pin_memory=True)\n",
        "\n",
        "# Initialize Model, Loss Function, and Optimizer\n",
        "model = ModifiedUNet(in_channels=1, out_channels=1).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10  # Define the number of epochs\n",
        "\n",
        "print(f\"Initial GPU memory allocated: {torch.cuda.memory_allocated()} bytes\")\n",
        "# Training loop\n",
        "scaler = GradScaler()\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        images = images.unsqueeze(0)  # This line seems unnecessary unless you're intentionally adding an extra dimension for some reason\n",
        "        optimizer.zero_grad()\n",
        "        # print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "\n",
        "        # Use autocast to enable mixed precision\n",
        "        with autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "        # Scale the loss and call backward\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Unscales the gradients and calls optimizer.step\n",
        "        scaler.step(optimizer)\n",
        "\n",
        "        # Updates the scale for next iteration\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if True:  # Always true, so this block runs for every batch\n",
        "          print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss/(batch_idx+1):.4f}\")\n",
        "          running_loss = 0.0\n",
        "          gc.collect()\n",
        "\n",
        "\n",
        "    # Evaluate on test and validation sets\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.0\n",
        "        val_loss = 0.0\n",
        "        for images, masks in test_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            test_loss += criterion(outputs, masks).item()\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            val_loss += criterion(outputs, masks).item()\n",
        "\n",
        "    print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "    # Save the model after each epoch\n",
        "    model_name = f\"modified_unet_epoch_{epoch + 1}.pt\"\n",
        "    model_path = os.path.join('saved_models', model_name)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved at {model_path}\")\n",
        "\n",
        "# Save the final model\n",
        "final_model_path = os.path.join('saved_models', \"modified_unet_final.pt\")\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"Final model saved at {final_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWih83iZqP5a",
        "outputId": "8b56a8a0-c7d8-4f44-c880-6f303e0f81fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 134 NIFTI files in /content/drive/MyDrive/ct_scan\n",
            "Valid count: 67\n",
            "Valid count: 67\n",
            "Valid count: 67\n",
            "Valid count: 67\n",
            "Number of workers: 0\n",
            "GPU memory allocated after operations: 0 bytes\n",
            "Initial GPU memory allocated: 0 bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}